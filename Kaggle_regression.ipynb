{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RMSLE\n",
    "import timeit\n",
    "import math\n",
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LET's make our scorer \n",
    "from sklearn import metrics\n",
    "rmsle_score = metrics.make_scorer(rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions fit for Kaggle\n",
    "# No negatives\n",
    "def fit_for_kaggle(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i] = max(0,y[i])\n",
    "# Only int\n",
    "    y = y.astype(int)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exports in Kaggle format\n",
    "def Export_for_Kaggle(y_pred, path):\n",
    "# path is the name of the file \"x.csv\"\n",
    "    data_out = pd.DataFrame(y_pred, columns = ['Prediction'])\n",
    "    data_out.index.name = 'Id'\n",
    "    data_out.to_csv(path, sep = \",\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_words_title</th>\n",
       "      <th>nb_words_content</th>\n",
       "      <th>pp_uniq_words</th>\n",
       "      <th>pp_uniq_non-stop_words</th>\n",
       "      <th>nb_links</th>\n",
       "      <th>nb_outside_links</th>\n",
       "      <th>nb_images</th>\n",
       "      <th>nb_videos</th>\n",
       "      <th>ave_word_length</th>\n",
       "      <th>nb_keywords</th>\n",
       "      <th>category</th>\n",
       "      <th>nb_mina_maxk</th>\n",
       "      <th>nb_mina_avek</th>\n",
       "      <th>nb_maxa_mink</th>\n",
       "      <th>nb_maxa_maxk</th>\n",
       "      <th>nb_maxa_avek</th>\n",
       "      <th>nb_avea_mink</th>\n",
       "      <th>nb_avea_maxk</th>\n",
       "      <th>nb_avea_avek</th>\n",
       "      <th>nb_min_linked</th>\n",
       "      <th>nb_max_linked</th>\n",
       "      <th>nb_ave_linked</th>\n",
       "      <th>weekday</th>\n",
       "      <th>dist_topic_0</th>\n",
       "      <th>dist_topic_1</th>\n",
       "      <th>dist_topic_2</th>\n",
       "      <th>dist_topic_3</th>\n",
       "      <th>dist_topic_4</th>\n",
       "      <th>subj</th>\n",
       "      <th>pp_pos_words</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>9</td>\n",
       "      <td>843</td>\n",
       "      <td>0.5358</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>369</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>843300</td>\n",
       "      <td>154500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4090</td>\n",
       "      <td>2422.0</td>\n",
       "      <td>850</td>\n",
       "      <td>20600</td>\n",
       "      <td>7750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20820</td>\n",
       "      <td>0.49280</td>\n",
       "      <td>0.02245</td>\n",
       "      <td>0.02246</td>\n",
       "      <td>0.25400</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0.04808</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>9</td>\n",
       "      <td>805</td>\n",
       "      <td>0.4196</td>\n",
       "      <td>0.5693</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>551</td>\n",
       "      <td>137</td>\n",
       "      <td>3200</td>\n",
       "      <td>843300</td>\n",
       "      <td>401400.0</td>\n",
       "      <td>1486</td>\n",
       "      <td>3483</td>\n",
       "      <td>2420.0</td>\n",
       "      <td>5600</td>\n",
       "      <td>5600</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.55070</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.29930</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>923</td>\n",
       "      <td>291</td>\n",
       "      <td>2500</td>\n",
       "      <td>843300</td>\n",
       "      <td>172300.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>4237</td>\n",
       "      <td>2854.0</td>\n",
       "      <td>3600</td>\n",
       "      <td>10100</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02223</td>\n",
       "      <td>0.02222</td>\n",
       "      <td>0.02223</td>\n",
       "      <td>0.02248</td>\n",
       "      <td>0.91080</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.03759</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>12</td>\n",
       "      <td>201</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>651</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>690400</td>\n",
       "      <td>98370.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3238</td>\n",
       "      <td>2225.0</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.11970</td>\n",
       "      <td>0.02003</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>0.72050</td>\n",
       "      <td>0.5091</td>\n",
       "      <td>0.07568</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>13</td>\n",
       "      <td>673</td>\n",
       "      <td>0.4609</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>819</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>843300</td>\n",
       "      <td>196100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4134</td>\n",
       "      <td>2363.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.69330</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.20670</td>\n",
       "      <td>0.4057</td>\n",
       "      <td>0.02757</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>7</td>\n",
       "      <td>293</td>\n",
       "      <td>0.6169</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>138</td>\n",
       "      <td>23900</td>\n",
       "      <td>843300</td>\n",
       "      <td>252600.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>5702</td>\n",
       "      <td>3969.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25900</td>\n",
       "      <td>0.02238</td>\n",
       "      <td>0.02267</td>\n",
       "      <td>0.02243</td>\n",
       "      <td>0.67350</td>\n",
       "      <td>0.5464</td>\n",
       "      <td>0.02612</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>11</td>\n",
       "      <td>254</td>\n",
       "      <td>0.6009</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1100</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>843300</td>\n",
       "      <td>177100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3607</td>\n",
       "      <td>1881.0</td>\n",
       "      <td>756</td>\n",
       "      <td>1200</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.86670</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.03334</td>\n",
       "      <td>0.5099</td>\n",
       "      <td>0.03896</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.4011</td>\n",
       "      <td>0.16000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>10</td>\n",
       "      <td>1034</td>\n",
       "      <td>0.4158</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>396</td>\n",
       "      <td>112</td>\n",
       "      <td>15000</td>\n",
       "      <td>843300</td>\n",
       "      <td>172400.0</td>\n",
       "      <td>1469</td>\n",
       "      <td>3286</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>932</td>\n",
       "      <td>1300</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.19990</td>\n",
       "      <td>0.71360</td>\n",
       "      <td>0.02857</td>\n",
       "      <td>0.02894</td>\n",
       "      <td>0.02890</td>\n",
       "      <td>0.4648</td>\n",
       "      <td>0.04711</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>0.3129</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>8</td>\n",
       "      <td>499</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>51.0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>801</td>\n",
       "      <td>356</td>\n",
       "      <td>10700</td>\n",
       "      <td>843300</td>\n",
       "      <td>337600.0</td>\n",
       "      <td>3470</td>\n",
       "      <td>49132</td>\n",
       "      <td>10310.0</td>\n",
       "      <td>2900</td>\n",
       "      <td>5000</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.35530</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.20440</td>\n",
       "      <td>0.4787</td>\n",
       "      <td>0.02484</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.5194</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>9</td>\n",
       "      <td>562</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.7048</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>335</td>\n",
       "      <td>8700</td>\n",
       "      <td>843300</td>\n",
       "      <td>451700.0</td>\n",
       "      <td>3253</td>\n",
       "      <td>13350</td>\n",
       "      <td>5861.0</td>\n",
       "      <td>988</td>\n",
       "      <td>20300</td>\n",
       "      <td>8696.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.83990</td>\n",
       "      <td>0.04000</td>\n",
       "      <td>0.04000</td>\n",
       "      <td>0.04000</td>\n",
       "      <td>0.04010</td>\n",
       "      <td>0.3972</td>\n",
       "      <td>0.04037</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.4188</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nb_words_title  nb_words_content  pp_uniq_words  pp_uniq_non-stop_words  \\\n",
       "2000               9               843         0.5358                  0.7469   \n",
       "2001               9               805         0.4196                  0.5693   \n",
       "2002               8               145         0.7594                  0.8488   \n",
       "2003              12               201         0.6359                  0.8148   \n",
       "2004              13               673         0.4609                  0.5950   \n",
       "2005               7               293         0.6169                  0.7225   \n",
       "2006              11               254         0.6009                  0.7481   \n",
       "2007              10              1034         0.4158                  0.5705   \n",
       "2008               8               499         0.5459                  0.6208   \n",
       "2009               9               562         0.5522                  0.7048   \n",
       "\n",
       "      nb_links  nb_outside_links  nb_images  nb_videos  ave_word_length  \\\n",
       "2000      15.0                 8         11          1                4   \n",
       "2001       8.0                 7          1          0                4   \n",
       "2002       7.0                 3          0          2                4   \n",
       "2003       7.0                 2          0          0                4   \n",
       "2004       8.0                 7          1          0                4   \n",
       "2005      24.0                22          1          0                5   \n",
       "2006       5.0                 3          0          0                4   \n",
       "2007       4.0                 0         15          1                4   \n",
       "2008      51.0                48          0          1                5   \n",
       "2009      23.0                20         10          1                4   \n",
       "\n",
       "      nb_keywords  category  nb_mina_maxk  nb_mina_avek  nb_maxa_mink  \\\n",
       "2000            9         1           369           150             0   \n",
       "2001            4         5           551           137          3200   \n",
       "2002            9         4           923           291          2500   \n",
       "2003           10         4           651           230             0   \n",
       "2004            6         5           819           185             0   \n",
       "2005            9         0           272           138         23900   \n",
       "2006            6         5          1100           315             0   \n",
       "2007            7         1           396           112         15000   \n",
       "2008            8         5           801           356         10700   \n",
       "2009            5         2          1000           335          8700   \n",
       "\n",
       "      nb_maxa_maxk  nb_maxa_avek  nb_avea_mink  nb_avea_maxk  nb_avea_avek  \\\n",
       "2000        843300      154500.0             0          4090        2422.0   \n",
       "2001        843300      401400.0          1486          3483        2420.0   \n",
       "2002        843300      172300.0          1145          4237        2854.0   \n",
       "2003        690400       98370.0             0          3238        2225.0   \n",
       "2004        843300      196100.0             0          4134        2363.0   \n",
       "2005        843300      252600.0          2890          5702        3969.0   \n",
       "2006        843300      177100.0             0          3607        1881.0   \n",
       "2007        843300      172400.0          1469          3286        2281.0   \n",
       "2008        843300      337600.0          3470         49132       10310.0   \n",
       "2009        843300      451700.0          3253         13350        5861.0   \n",
       "\n",
       "      nb_min_linked  nb_max_linked  nb_ave_linked  weekday  dist_topic_0  \\\n",
       "2000            850          20600         7750.0        1       0.20820   \n",
       "2001           5600           5600         5600.0        2       0.05000   \n",
       "2002           3600          10100         6850.0        0       0.02223   \n",
       "2003           2400           2400         2400.0        2       0.11980   \n",
       "2004          20000          20000        20000.0        2       0.03333   \n",
       "2005          10000          10000        10000.0        3       0.25900   \n",
       "2006            756           1200          978.0        0       0.03333   \n",
       "2007            932           1300         1116.0        4       0.19990   \n",
       "2008           2900           5000         3950.0        2       0.02500   \n",
       "2009            988          20300         8696.0        2       0.83990   \n",
       "\n",
       "      dist_topic_1  dist_topic_2  dist_topic_3  dist_topic_4    subj  \\\n",
       "2000       0.49280       0.02245       0.02246       0.25400  0.5108   \n",
       "2001       0.05000       0.55070       0.05000       0.29930  0.4958   \n",
       "2002       0.02222       0.02223       0.02248       0.91080  0.5364   \n",
       "2003       0.11970       0.02003       0.02000       0.72050  0.5091   \n",
       "2004       0.03333       0.69330       0.03333       0.20670  0.4057   \n",
       "2005       0.02238       0.02267       0.02243       0.67350  0.5464   \n",
       "2006       0.03333       0.86670       0.03333       0.03334  0.5099   \n",
       "2007       0.71360       0.02857       0.02894       0.02890  0.4648   \n",
       "2008       0.35530       0.02500       0.39030       0.20440  0.4787   \n",
       "2009       0.04000       0.04000       0.04000       0.04010  0.3972   \n",
       "\n",
       "      pp_pos_words  pp_neg_words  pp_pos_words_in_nonneutral  ave_polar_pos  \\\n",
       "2000       0.04808      0.019230                      0.7143         0.4437   \n",
       "2001       0.02956      0.025710                      0.5349         0.3081   \n",
       "2002       0.03759      0.007519                      0.8333         0.3673   \n",
       "2003       0.07568      0.027030                      0.7368         0.3721   \n",
       "2004       0.02757      0.021440                      0.5625         0.3500   \n",
       "2005       0.02612      0.011190                      0.7000         0.2964   \n",
       "2006       0.03896      0.030300                      0.5625         0.4011   \n",
       "2007       0.04711      0.010790                      0.8136         0.3129   \n",
       "2008       0.02484      0.006211                      0.8000         0.5194   \n",
       "2009       0.04037      0.027520                      0.5946         0.4188   \n",
       "\n",
       "      min_polar_pos  max_polar_pos  \n",
       "2000        0.03333            1.0  \n",
       "2001        0.05000            0.8  \n",
       "2002        0.13640            0.5  \n",
       "2003        0.13640            0.6  \n",
       "2004        0.05000            0.6  \n",
       "2005        0.10000            0.7  \n",
       "2006        0.16000            0.5  \n",
       "2007        0.10000            0.7  \n",
       "2008        0.03333            0.9  \n",
       "2009        0.10000            1.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import all data\n",
    "feature_data = pd.read_csv('kaggle_data/features.txt', header=None, sep=\"  \", names=['feature_names', 'feature_description'], engine='python')\n",
    "list_feature_names = list(feature_data['feature_names'])\n",
    "train_data = pd.read_csv('kaggle_data/train.csv', header=None, sep=\" \", names=list_feature_names)\n",
    "test_data=pd.read_csv('kaggle_data/test-val.csv',header=None,sep=\" \",names=list_feature_names)\n",
    "target_data = pd.read_csv('kaggle_data/train-targets.csv', sep=\",\")\n",
    "train_data = train_data.drop(['pp_stop_words','ave_polar_neg','min_polar_neg','max_polar_neg','polar_title','subj_title','polar','nb_mina_mink'],axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data engineering \n",
    "test_data = test_data.drop(['pp_stop_words','ave_polar_neg','min_polar_neg','max_polar_neg','polar_title','subj_title','polar','nb_mina_mink'],axis=1)\n",
    "# 'weekday' encoding\n",
    "weekday_data = pd.get_dummies(train_data['weekday'],prefix='weekday',drop_first=True)\n",
    "# 'category' encoding\n",
    "category_data = pd.get_dummies(train_data['category'],prefix='category',drop_first=True)\n",
    "#fusion that shit\n",
    "other_data = train_data.drop(['weekday','category'],axis=1)\n",
    "training_data = pd.concat([category_data,weekday_data,other_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets do the same with the test data\n",
    "weekday_data_test=pd.get_dummies(test_data['weekday'],prefix='weekday',drop_first=True)\n",
    "category_data_test=pd.get_dummies(test_data['category'],prefix='category',drop_first=True)\n",
    "other_data_test=test_data.drop(['weekday','category'],axis=1)\n",
    "testing_data=pd.concat([category_data_test,weekday_data_test,other_data_test],axis=1)\n",
    "#see it\n",
    "#testing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.12860834 -0.43740958 -0.25487787 ...,  0.85841924 -0.84504969\n",
      "   0.98277162]\n",
      " [-0.46979051 -0.43740958 -0.25487787 ..., -0.43987772 -0.61279765\n",
      "   0.17296496]\n",
      " [-0.46979051 -0.43740958 -0.25487787 ...,  0.1269304   0.59095615\n",
      "  -1.04174503]\n",
      " ..., \n",
      " [-0.46979051 -0.43740958 -0.25487787 ..., -0.16604812  0.08381913\n",
      "   0.98277162]\n",
      " [-0.46979051 -0.43740958 -0.25487787 ..., -3.38976925 -1.30941444\n",
      "  -3.06626168]\n",
      " [-0.46979051 -0.43740958 -0.25487787 ...,  1.02118508  0.08381913\n",
      "   0.98277162]]\n"
     ]
    }
   ],
   "source": [
    "#Data standardization\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(training_data)\n",
    "X_reg = scaler.transform(training_data)\n",
    "y_reg = target_data['Prediction'].values\n",
    "\n",
    "#scale that test data\n",
    "X_test = scaler.transform(testing_data)\n",
    "\n",
    "#visualize X_reg\n",
    "print X_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the kfold\n",
    "from sklearn import model_selection\n",
    "kf = model_selection.KFold(n_splits = 10)\n",
    "k_folds = kf.split(X_reg, y_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the Linear regression\n",
    "from sklearn import linear_model\n",
    "lin_reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2867, 2237, 4349, ..., 3674, 6793, 1897])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions by Cross validation\n",
    "y_pred = model_selection.cross_val_predict(lin_reg, X_reg, y_reg, cv=k_folds)\n",
    "# num shares can't be negative, so 0 instead\n",
    "fit_for_kaggle(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 60429191.508\n"
     ]
    }
   ],
   "source": [
    "# Let's see the error\n",
    "from sklearn import metrics\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_reg, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2177093989935257"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSLE \n",
    "rmsle(y_reg, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3159.6075969   4992.74842606  1855.4661532  ...,  2680.83242045\n",
      "  1919.90795272  3575.68864659]\n"
     ]
    }
   ],
   "source": [
    "# Prediction on Test set\n",
    "lin_reg.fit(X_reg,y_reg)\n",
    "y_test_reg = lin_reg.predict(X_test)\n",
    "fit_for_kaggle(y_test_reg)\n",
    "print(y_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expoeting\n",
    "Export_for_Kaggle(y_test_reg, \"solution_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.055437278734848"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RIDGE\n",
    "kf2 = model_selection.KFold(n_splits=5)\n",
    "kf2.get_n_splits(X_reg)\n",
    "folds_regr = [(tr, te) for (tr, te) in kf2.split(X_reg)]\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 6)}\n",
    "#regr_ridge_opt = model_selection.GridSearchCV(linear_model.Ridge(), param_grid, cv=folds_regr, scoring='neg_mean_squared_log_error')\n",
    "regr_ridge_opt = model_selection.GridSearchCV(linear_model.Ridge(), param_grid)\n",
    "regr_ridge_opt.fit(X_reg, y_reg)\n",
    "ypred_ridge_opt = regr_ridge_opt.predict(X_reg)\n",
    "fit_for_kaggle(ypred_ridge_opt)\n",
    "rmsle(y_reg, ypred_ridge_opt)\n",
    "#print('param=', regr_ridge_opt.best_params_, 'RMSE=', np.sqrt(-1*regr_ridge_opt.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ridge for test\n",
    "y_test_ridge = regr_ridge_opt.predict(X_test)\n",
    "y_test_ridge = fit_for_kaggle(y_test_ridge)\n",
    "Export_for_Kaggle(y_test_ridge, \"solution_ridge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "regr_lasso = model_selection.GridSearchCV(linear_model.Lasso(), param_grid, cv=folds_regr)\n",
    "regr_lasso.fit(X_reg, y_reg)\n",
    "ypred_lasso = regr_lasso.predict(X_test)\n",
    "ypred_lasso = fit_for_kaggle(ypred_lasso)\n",
    "Export_for_Kaggle(ypred_lasso, \"solution_lasso.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-48786556d0eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#let's declare it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mt_SNE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_reg_tsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_SNE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reg_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mX_test_tsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_SNE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    857\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m--> 859\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    771\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36m_tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, random_state, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m    813\u001b[0m         \u001b[0mP\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[1;32m--> 815\u001b[1;33m                                                       **opt_args)\n\u001b[0m\u001b[0;32m    816\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tSNE\n",
    "from sklearn import manifold\n",
    "#let's make a copy of X_reg\n",
    "X_reg_1 = X_reg\n",
    "#let's declare it\n",
    "t_SNE = manifold.TSNE(n_components = 3)\n",
    "X_reg_tsne = t_SNE.fit_transform(X_reg_1, y_reg)\n",
    "X_test_tsne = t_SNE.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn import decomposition\n",
    "ACP = decomposition.PCA(n_components = 40)\n",
    "ACP.fit(X_reg)\n",
    "X_test_acp = ACP.transform(X_test)\n",
    "X_reg_acp = ACP.transform(X_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN again\n",
    "# creating odd list of K for KNN\n",
    "list_neighbors = list(range(1,50))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in list_neighbors:\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "    scores = model_selection.cross_val_score(knn, X_reg, y_reg, cv=10, scoring=rmsle_score)\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15fc9f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_neighbors, cv_scores)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print cv_scores.index(max(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's find the best PCA\n",
    "PCAList = list(range(2,52))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores_pca = []\n",
    "knn_cv = neighbors.KNeighborsRegressor(n_neighbors=14)\n",
    "# perform 10-fold cross validation\n",
    "for k in PCAList:\n",
    "    ACP_cv = decomposition.PCA(n_components = k)\n",
    "    ACP_cv.fit(X_reg)\n",
    "    X_reg_cv = ACP_cv.transform(X_reg)\n",
    "    scores = model_selection.cross_val_score(knn, X_reg_cv, y_reg, cv=10, scoring='neg_mean_squared_log_error')\n",
    "    cv_scores_pca.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJxoUAREZG4UKCIiEhc/H+/gZ6KiiVXkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x165ad7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(PCAList, cv_scores_pca)\n",
    "plt.xlabel('n PCA')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print cv_scores_pca.index(max(cv_scores_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's fit that shiiiiiit\n",
    "knn_14 = neighbors.KNeighborsRegressor(14)\n",
    "PCA_47 = decomposition.PCA(47)\n",
    "PCA_47.fit(X_reg)\n",
    "X_reg_47 = PCA_47.transform(X_reg)\n",
    "X_test_47 = PCA_47.transform(X_test)\n",
    "knn_14.fit(X_reg_47, y_reg)\n",
    "y_test_14_47 = knn_14.predict(X_test_47)\n",
    "y_test_14_47 = fit_for_kaggle(y_test_14_47)\n",
    "Export_for_Kaggle(y_test_14_47, \"solution_knn_14_47.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# double cross validation\n",
    "'''ATTENTION PREND ENORMEMENT DE TEMPS A CALCULER\n",
    "    NE PAS EXECUTER'''\n",
    "pcaList = list(range(2,52))\n",
    "knnList = list(range(1,50))\n",
    "cv_scores_db = []\n",
    "cv_rmsle = np.zeros((50,49))\n",
    "for i in pcaList :\n",
    "    for j in knnList : \n",
    "        ACP_db = decomposition.PCA(n_components = i)\n",
    "        ACP_db.fit(X_reg)\n",
    "        X_reg_db = ACP_db.transform(X_reg)  \n",
    "        knn = neighbors.KNeighborsRegressor(n_neighbors=j)\n",
    "        #scores = model_selection.cross_val_score(knn, X_reg_db, y_reg, cv=10, scoring='neg_mean_squared_log_error')\n",
    "        pred = model_selection.cross_val_predict(knn, X_reg_db, y_reg, cv=10, n_jobs=-1)\n",
    "        #cv_scores_db.append(scores.mean())\n",
    "        cv_rmsle[i-2,j-1] = rmsle(y_reg, pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's try it \n",
    "knn_27 = neighbors.KNeighborsRegressor(13)\n",
    "PCA_48 = decomposition.PCA(49)\n",
    "PCA_48.fit(X_reg)\n",
    "X_reg_48 = PCA_49.transform(X_reg)\n",
    "X_test_48 = PCA_49.transform(X_test)\n",
    "knn_27.fit(X_reg_48, y_reg)\n",
    "y_test_27_48 = knn_27.predict(X_test_48)\n",
    "y_test_27_48 = fit_for_kaggle(y_test_27_48)\n",
    "Export_for_Kaggle(y_test_27_48, \"solution_knn_13_49.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lasso with PCA\n",
    "regr_lasso_pca = model_selection.GridSearchCV(linear_model.Lasso(), param_grid, cv=folds_regr)\n",
    "regr_lasso_pca.fit(X_reg_48, y_reg)\n",
    "ypred_lasso_48 = regr_lasso_pca.predict(X_test_48)\n",
    "ypred_lasso_48 = fit_for_kaggle(ypred_lasso_48)\n",
    "Export_for_Kaggle(ypred_lasso_48, \"solution_lasso_48.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn import ensemble\n",
    "#PCA as always\n",
    "PCA = decomposition.PCA(49)\n",
    "PCA.fit(X_reg)\n",
    "X_reg_pca = PCA.transform(X_reg)\n",
    "X_test_pca = PCA.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-26f73c67a9d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrandomf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_log_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrandomf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reg_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my_test_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandomf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_pca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0my_test_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_for_kaggle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_rf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 638\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 327\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Grid Search the random Forest\n",
    "param_grid = {'criterion': ['mse','mae'],\n",
    "             'min_samples_leaf' : [1, 2, 5],\n",
    "             'n_estimators': [5, 10, 20, 50, 80]}\n",
    "\n",
    "randomf = model_selection.GridSearchCV(ensemble.RandomForestRegressor(), param_grid, cv=10, scoring='neg_mean_squared_log_error')\n",
    "randomf.fit(X_reg_pca, y_reg)\n",
    "y_test_rf = randomf.predict(X_test_pca)\n",
    "y_test_rf = fit_for_kaggle(y_test_rf)\n",
    "Export_for_Kaggle(y_test_rf, \"solution_RandomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just random forest\n",
    "randomforest = ensemble.RandomForestRegressor(n_estimators = 50, max_features='sqrt')\n",
    "rf_scores = model_selection.cross_val_score(randomforest, X_reg_pca, y_reg, cv=10, scoring = rmsle_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08466542 -1.10300477 -1.1024676  -1.03267521 -1.0778305  -1.08842206\n",
      " -1.16990584 -1.10620081 -1.11994094 -1.09107425]\n"
     ]
    }
   ],
   "source": [
    "print rf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try KBEST\n",
    "from sklearn import feature_selection\n",
    "kbest = feature_selection.SelectKBest(feature_selection.f_regression, k=40)\n",
    "kbest.fit(X_reg, y_reg)\n",
    "X_reg_best = kbest.transform(X_reg)\n",
    "X_test_best = kbest.transform(X_test)\n",
    "kNN = neighbors.KNeighborsRegressor(44)\n",
    "kNN.fit(X_reg_best, y_reg)\n",
    "y_kNN = kNN.predict(X_test_best)\n",
    "y_kNN = fit_for_kaggle(y_kNN)\n",
    "Export_for_Kaggle(y_kNN,'solution_kbest_knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-c00820bb7a4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mKBestList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mkbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mkbest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mX_reg_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkbest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tariq\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.pyc\u001b[0m in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "#Cross validate Kbest\n",
    "# creating odd list of K for KNN\n",
    "KBestList = list(range(1,52))\n",
    "# empty list that will hold cv scores\n",
    "cv_scores_kbest = []\n",
    "# perform 10-fold cross validation\n",
    "for l in KBestList:\n",
    "    kbest = feature_selection.SelectKBest(feature_selection.chi2, k=l)\n",
    "    kbest.fit(X_reg, y_reg)\n",
    "    X_reg_best = kbest.transform(X_reg)\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=14)\n",
    "    scores = model_selection.cross_val_score(knn, X_reg_best, y_reg, cv=10, scoring=rmsle_score)\n",
    "    cv_scores_kbest.append(scores.mean())\n",
    "\n",
    "plt.plot(KBestList, cv_scores_kbest)\n",
    "plt.xlabel('n K')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n",
    "print cv_scores_kbest.index(max(cv_scores_kbest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
